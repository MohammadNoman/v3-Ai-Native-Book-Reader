## 5.1 System Architecture

Before we write code, we define the architecture. Our RAG (Retrieval-Augmented Generation) system consists of an ingestion pipeline to process our specs and a query engine to serve answers.

<div className="bg-slate-900 p-6 rounded-lg my-6 border border-slate-700 font-mono text-xs overflow-x-auto">
{`+----------------+       +-------------------+       +--------------------+
|  Doc Sources   | ----> | Ingestion Pipeline| ----> |  Qdrant Vector DB  |
| (Markdown/MDX) |       | (Chunk -> Embed)  |       | (Semantic Index)   |
+----------------+       +-------------------+       +--------------------+
                                                             ^
                                                             | Retrieval
+----------------+       +-------------------+       +--------------------+
|   Frontend     | <---> |   FastAPI App     | <---> |   Gemini 2.5 Agent |
| (Chat Widget)  |       | (Agent Router)    |       | (Reasoning Engine) |
+----------------+       +-------------------+       +--------------------+`}
</div>

## 5.2 Dependency Management

We use `poetry` or `pip` to manage our Python environment. Create a `pyproject.toml` file:

<div className="bg-slate-900 p-6 rounded-lg my-6 border border-slate-700">
  <h4 className="text-brand-400 font-bold mb-2">pyproject.toml</h4>
  <pre className="text-xs text-slate-300 overflow-x-auto">
{`[tool.poetry]
name = "ai-native-rag"
version = "0.1.0"
description = "RAG Backend for Doc Platform"
authors = ["AI Architect <arch@example.com>"]

[tool.poetry.dependencies]
python = "^3.10"
fastapi = "^0.109.0"
uvicorn = "^0.27.0"
qdrant-client = "^1.7.0"
google-genai = "^0.3.0"
pydantic = "^2.6.0"
python-dotenv = "^1.0.0"

[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"`}
  </pre>
</div>

## 5.3 The Ingestion Pipeline

This script loads your Docusaurus Markdown files, splits them into semantic chunks, generates embeddings using `text-embedding-004`, and upserts them into Qdrant.

<div className="bg-slate-900 p-6 rounded-lg my-6 border border-slate-700">
  <h4 className="text-brand-400 font-bold mb-2">src/ingest.py</h4>
  <pre className="text-xs text-slate-300 overflow-x-auto">
{`import os
import glob
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct
from google.genai import GoogleGenAI
from dotenv import load_dotenv

load_dotenv()

# Configuration
QDRANT_URL = os.getenv("QDRANT_URL")
QDRANT_KEY = os.getenv("QDRANT_API_KEY")
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
COLLECTION_NAME = "doc_specs"

# Initialize Clients
qdrant = QdrantClient(url=QDRANT_URL, api_key=QDRANT_KEY)
ai = GoogleGenAI(api_key=GOOGLE_API_KEY)

def init_db():
    qdrant.recreate_collection(
        collection_name=COLLECTION_NAME,
        vectors_config=VectorParams(size=768, distance=Distance.COSINE),
    )

def embed_text(text: str):
    result = ai.models.embed_content(
        model="models/text-embedding-004",
        content=text,
        task_type="retrieval_document"
    )
    return result.embedding.values

def ingest_docs(docs_path: str):
    init_db()
    files = glob.glob(f"{docs_path}/**/*.md", recursive=True)
    points = []
    idx = 0
    
    for file_path in files:
        with open(file_path, "r") as f:
            content = f.read()
            # Simple chunking by header (naive approach)
            chunks = content.split("## ")
            
            for chunk in chunks:
                if len(chunk.strip()) < 50: continue
                
                vector = embed_text(chunk)
                points.append(PointStruct(
                    id=idx,
                    vector=vector,
                    payload={"source": file_path, "text": chunk[:1000]}
                ))
                idx += 1
                
                if len(points) >= 50:
                    qdrant.upsert(collection_name=COLLECTION_NAME, points=points)
                    points = []
                    print(f"Indexed {idx} chunks...")
                    
    if points:
        qdrant.upsert(collection_name=COLLECTION_NAME, points=points)

if __name__ == "__main__":
    ingest_docs("../docs")`}
  </pre>
</div>

## 5.4 The Agent API (FastAPI)

This is the core brain. We expose an endpoint `/ask` that accepts a query and optional selected text context.

<div className="bg-slate-900 p-6 rounded-lg my-6 border border-slate-700">
  <h4 className="text-brand-400 font-bold mb-2">src/main.py</h4>
  <pre className="text-xs text-slate-300 overflow-x-auto">
{`from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from qdrant_client import QdrantClient
from google.genai import GoogleGenAI, types
import os
from dotenv import load_dotenv

load_dotenv()

app = FastAPI()
ai = GoogleGenAI(api_key=os.getenv("GOOGLE_API_KEY"))
qdrant = QdrantClient(
    url=os.getenv("QDRANT_URL"), 
    api_key=os.getenv("QDRANT_API_KEY")
)
COLLECTION_NAME = "doc_specs"

# --- Tools ---
def search_knowledge_base(query: str):
    """Retrieves relevant documentation segments based on semantic query."""
    embedding = ai.models.embed_content(
        model="models/text-embedding-004",
        content=query,
        task_type="retrieval_query"
    ).embedding.values
    
    hits = qdrant.search(
        collection_name=COLLECTION_NAME,
        query_vector=embedding,
        limit=3
    )
    return "\\n".join([f"Source: {h.payload['source']}\\nContent: {h.payload['text']}" for h in hits])

search_tool = types.Tool(
    function_declarations=[
        types.FunctionDeclaration(
            name="search_knowledge_base",
            description="Look up technical specifications and documentation.",
            parameters=types.Schema(
                type=types.Type.OBJECT,
                properties={
                    "query": types.Schema(type=types.Type.STRING)
                },
                required=["query"]
            )
        )
    ]
)

# --- Endpoints ---
class ChatRequest(BaseModel):
    message: str
    selected_context: str | None = None

@app.post("/ask")
async def ask_agent(req: ChatRequest):
    system_instruction = "You are a Technical Assistant for an AI Platform. Use the search tool to find answers."
    
    if req.selected_context:
        system_instruction += f"\\nThe user has highlighted this code/text: '{req.selected_context}'. Focus your answer on this context."
    
    # Generate content with tools
    response = await ai.models.generate_content(
        model="gemini-2.5-flash",
        contents=req.message,
        config=types.GenerateContentConfig(
            tools=[search_tool],
            system_instruction=system_instruction
        )
    )
    
    # Handle Tool calls automatically (simplified for brevity)
    # In production, you would loop through parts, execute function calls, and send function response back.
    # For this snippet, we assume the model might return a tool call we need to handle or direct text.
    
    return {"reply": response.text or "I need to check the docs..."}

@app.get("/health")
def health():
    return {"status": "ok"}`}
  </pre>
</div>
